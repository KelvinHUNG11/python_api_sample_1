{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_id</th>\n",
       "      <th>channel</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>content</th>\n",
       "      <th>comment_time</th>\n",
       "      <th>ref_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VMfwN-XHiJs</td>\n",
       "      <td>youtube</td>\n",
       "      <td>UgybSDK1DreyYAP3il14AaABAg</td>\n",
       "      <td>å¯èƒ½ä»–å–œæ­¡åˆ¥äººğŸ˜­</td>\n",
       "      <td>2021-05-27 08:40:26+00</td>\n",
       "      <td>2022-05-05 22:41:48.966+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VMfwN-XHiJs</td>\n",
       "      <td>youtube</td>\n",
       "      <td>Ugywq8IS5gNcfn-G7GV4AaABAg</td>\n",
       "      <td>ä½œè€…ï¼Œé‚£ä»–æ˜¯å¦æœƒåšå‡ºä¸€äº›è®“ä½ ä»¥ç‚ºä»–å–œæ­¡ä½ çš„è¡Œç‚ºï¼Œä½†æ˜¯å…¶å¯¦ä»–ä¸å–œæ­¡ä½ åªæ˜¯ä½ è‡ªå·±è¦ºå¾—è€Œå·²ï¼Ÿ</td>\n",
       "      <td>2021-05-27 08:40:26+00</td>\n",
       "      <td>2022-05-05 22:41:48.966+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VMfwN-XHiJs</td>\n",
       "      <td>youtube</td>\n",
       "      <td>Ugy2llUeMTATQRck0-t4AaABAg</td>\n",
       "      <td>æå¥³è¡¨ç¤ºOTZ</td>\n",
       "      <td>2021-05-27 08:40:26+00</td>\n",
       "      <td>2022-05-05 22:41:48.966+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VMfwN-XHiJs</td>\n",
       "      <td>youtube</td>\n",
       "      <td>UgyYuZLLlYOaqLq39iF4AaABAg</td>\n",
       "      <td>ä¸€å †è¦å‰‡çœŸçš„å¾ˆå¿«å°±å†·æ‰</td>\n",
       "      <td>2021-05-27 08:40:26+00</td>\n",
       "      <td>2022-05-05 22:41:48.966+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VMfwN-XHiJs</td>\n",
       "      <td>youtube</td>\n",
       "      <td>Ugwf76BqnP6zE-qnlu54AaABAg</td>\n",
       "      <td>å¯èƒ½ä»–æœ‰å¥³å‹ã€‚æˆ–å·²ç¶“çµå©šã€‚ ^___^</td>\n",
       "      <td>2021-05-27 08:40:26+00</td>\n",
       "      <td>2022-05-05 22:41:48.966+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    content_id  channel                  comment_id  \\\n",
       "0  VMfwN-XHiJs  youtube  UgybSDK1DreyYAP3il14AaABAg   \n",
       "1  VMfwN-XHiJs  youtube  Ugywq8IS5gNcfn-G7GV4AaABAg   \n",
       "2  VMfwN-XHiJs  youtube  Ugy2llUeMTATQRck0-t4AaABAg   \n",
       "3  VMfwN-XHiJs  youtube  UgyYuZLLlYOaqLq39iF4AaABAg   \n",
       "4  VMfwN-XHiJs  youtube  Ugwf76BqnP6zE-qnlu54AaABAg   \n",
       "\n",
       "                                       content            comment_time  \\\n",
       "0                                     å¯èƒ½ä»–å–œæ­¡åˆ¥äººğŸ˜­  2021-05-27 08:40:26+00   \n",
       "1  ä½œè€…ï¼Œé‚£ä»–æ˜¯å¦æœƒåšå‡ºä¸€äº›è®“ä½ ä»¥ç‚ºä»–å–œæ­¡ä½ çš„è¡Œç‚ºï¼Œä½†æ˜¯å…¶å¯¦ä»–ä¸å–œæ­¡ä½ åªæ˜¯ä½ è‡ªå·±è¦ºå¾—è€Œå·²ï¼Ÿ  2021-05-27 08:40:26+00   \n",
       "2                                      æå¥³è¡¨ç¤ºOTZ  2021-05-27 08:40:26+00   \n",
       "3                                  ä¸€å †è¦å‰‡çœŸçš„å¾ˆå¿«å°±å†·æ‰  2021-05-27 08:40:26+00   \n",
       "4                          å¯èƒ½ä»–æœ‰å¥³å‹ã€‚æˆ–å·²ç¶“çµå©šã€‚ ^___^  2021-05-27 08:40:26+00   \n",
       "\n",
       "                 ref_datetime  \n",
       "0  2022-05-05 22:41:48.966+00  \n",
       "1  2022-05-05 22:41:48.966+00  \n",
       "2  2022-05-05 22:41:48.966+00  \n",
       "3  2022-05-05 22:41:48.966+00  \n",
       "4  2022-05-05 22:41:48.966+00  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "##Read the sample file from local storage\n",
    "sample_data=pd.read_csv('../sample_comment.csv')\n",
    "sample_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Convert all the \"content_id\" and \"comment_id\" to string for analysis later\n",
    "comment_df = pd.DataFrame(sample_data, columns=[\"content_id\", \"channel\", \"comment_id\", \"content\", \"comment_time\", \"ref_datetime\" ])\n",
    "comment_df[\"content_id\"], comment_df[\"comment_id\"] = comment_df[\"content_id\"].apply(str), comment_df[\"comment_id\"].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "import emoji\n",
    "NUM_PROC = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @calculate_time_cost\n",
    "def label_with_emoji(comment_df, EMOJI_CSV):\n",
    "    emoji_df = pd.read_csv(EMOJI_CSV, encoding=\"utf-8-sig\")\n",
    "    senti_re_df = emoji_df.groupby(\"sentiment\").agg({\"re_emoji\": lambda cell: \"|\".join(cell)})\n",
    "    pos_emoji_re = senti_re_df.loc[\"pos\", \"re_emoji\"]\n",
    "    neu_emoji_re = senti_re_df.loc[\"neu\", \"re_emoji\"]\n",
    "    neg_emoji_re = senti_re_df.loc[\"neg\", \"re_emoji\"]\n",
    "\n",
    "    for comment_df_index in comment_df[comment_df[\"pre_sentiment\"].isna()].index:\n",
    "        senti_sum = 0\n",
    "        found_flag = False\n",
    "        for emoji_re, coefficient in zip([pos_emoji_re, neu_emoji_re, neg_emoji_re], [1, 0, -1]):\n",
    "            senti_sum += (num_found_emoji:=len(re.findall(emoji_re, comment_df.loc[comment_df_index, \"content\"]))) * coefficient\n",
    "            if num_found_emoji: found_flag = True\n",
    "        if found_flag:\n",
    "            if senti_sum>0: senti = \"pos\"\n",
    "            elif senti_sum<0: senti = \"neg\"\n",
    "            else: senti = \"neu\"\n",
    "            comment_df.loc[comment_df_index, [\"pre_sentiment\", \"label_reason\"]] = (senti, \"emoji\")\n",
    "            comment_df.loc[comment_df_index, senti] = 1\n",
    "\n",
    "\n",
    "    return comment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_preprocessing_with_content_col(comment_df):\n",
    "    # print(\"string_preprocessing_with_content_col\")\n",
    "    REGEX_EMAIL = re.compile(r\"[a-zA-Z0-9_\\.-]+@[a-zA-Z0-9_-]+(?:\\.[a-zA-Z0-9_-]+)+\")\n",
    "    REGEX_HTTP  = re.compile(r'https?:/?/?[a-zA-Z0-9.?%+/&#=:_-]*')                                         # http(s)://website +for search, %for Chinese, #for topic \n",
    "    REGEX_WWW   = re.compile(r\"[^a-zA-Z0-9/]www\\.[a-zA-Z0-9.?%+/&#=:_-]+|^www\\.[a-zA-Z0-9.?%+/&#=:_-]+\")    # www.website\n",
    "    REGEX_COM   = re.compile(r\"[a-zA-Z0-9?%+/&#=:_-]+\\.com[a-zA-Z0-9.?%+/&#=:_-]+\")                         # website.com.website\n",
    "    REGEX_CODE = re.compile(r\"&#+[0-9]+;\")\n",
    "    REGEX_2SPACE = re.compile(r\"\\n|\\r|\\t\\f\\v\")\n",
    "    REGEX_2COMMA = re.compile(r\"ã€|ï¼Œ|ï¼›\")\n",
    "    REGEX_2Quot = re.compile(r\"ã€Š|ã€‹|â€œ|â€|â€˜|â€™|ã€Œ|ã€\")\n",
    "    REGEX_IGEMOJI =  re.compile(\":_\\w+:\")\n",
    "    \n",
    "    PUNC = '~`!#$%^&*()_+-=|\\';\":/.,?><~Â·ï¼@#ï¿¥%â€¦â€¦&*ï¼ˆï¼‰â€”â€”+-=â€œï¼šâ€™ï¼›ã€ã€‚ï¼Œï¼Ÿã€‹ã€Š{}'\n",
    "    #for index in comment_df[comment_df[\"pre_sentiment\"].isna()].index:\n",
    "    for index in comment_df.index:\n",
    "        text = str(comment_df.loc[index, \"content\"])\n",
    "        text = text.replace('\"',\"\")\n",
    "        text = text.encode('unicode-escape').decode('unicode-escape')\n",
    "        #print('*'*10,text)\n",
    "        text = re.sub(REGEX_EMAIL, 'EMAIL', text)\n",
    "        text = re.sub(REGEX_HTTP, 'HTTP', text)\n",
    "        text = re.sub(REGEX_WWW, 'HTTP', text)\n",
    "        text = re.sub(REGEX_COM, 'HTTP', text)\n",
    "        text = re.sub(REGEX_CODE, \"\", text)                         # é‡åˆ°éƒ¨åˆ†ç‰¹æ®Šå­—ç¬¦æ— æ³•è¯†åˆ«\n",
    "        #text = emojiswitch.demojize(text, delimiters = (\" \",\" \"), lang=\"en\").replace(\"_\", \" \").replace(\"-\", \" \")\n",
    "        text = emoji.replace_emoji(text, ' ')\n",
    "        text = re.sub(REGEX_IGEMOJI, \"\", text)\n",
    "        text = re.sub(r\"(å›å¤)?(//)?\\s*@\\S*?\\s*( |:|ï¼š|$)\", \"\", text)  # å»é™¤æ­£æ–‡ä¸­çš„@å’Œå›å¤/è½¬å‘ä¸­çš„ç”¨æˆ·å\n",
    "        text = re.sub(REGEX_2SPACE, \" \", text)\n",
    "        text = re.sub(REGEX_2COMMA,\",\", text)\n",
    "        text = re.sub(REGEX_2Quot, \"'\", text)\n",
    "        #text = text.replace(\"ï¼\",\"!\").replace(\"ï¼Ÿ\",\"?\").replace(\"ã€‚\",\".\")\n",
    "        #text = re.sub(r'[0-9]+', \"NUMBER\", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)                                # åˆå¹¶æ­£æ–‡ä¸­è¿‡å¤šçš„ç©ºæ ¼\n",
    "        text = text.replace(\"ã€‚\",\".\")\n",
    "        text = re.sub(r\"[%s]+\" %PUNC, \"\",text)\n",
    "        text = text.strip()                                             # å»æ‰å­—ç¬¦ä¸²çš„å·¦å³ä¸¤è¾¹ç©ºæ ¼\n",
    "        comment_df.loc[index, \"clean_content\"] = text\n",
    "    return comment_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Pre-processing of imported comments\n",
    "def pre_processing_and_pre_predict(comment_df):\n",
    "    if comment_df.empty:\n",
    "        return comment_df\n",
    "    # print(\"original shape\", comment_df.shape)\n",
    "    '0. initialization for sentiment'\n",
    "    comment_df[\"pre_sentiment\"] = np.nan\n",
    "    comment_df[\"pre_sentiment\"] = comment_df[\"pre_sentiment\"].astype(\"object\")\n",
    "\n",
    "    comment_df['label_reason'] = None\n",
    "    comment_df[['pos', 'neu', 'neg']] = 0,0,0\n",
    "    # print('aaa comment_df[\"pre_sentiment\"]', comment_df[\"pre_sentiment\"])\n",
    "    '''\n",
    "    # sub_comment will be skipped \n",
    "    '1. Take off all subcomment from comment, then concat comments and subcomments as one.'\n",
    "    comment_df = takeout_sub_comment_as_comment(comment_df)\n",
    "    # print(\"turn_sub_comment_into_comment finished.\")\n",
    "    logging.info(\"turn_sub_comment_into_comment finished.\")\n",
    "    # print(\"adding subcomment\", comment_df.shape)\n",
    "    '''\n",
    "    \n",
    "    ''' **********************HOPE sn will be used later******************\n",
    "    '2. Labelling sentiment directly according to the interact info.'\n",
    "    comment_df = fun_sn_parsing(comment_df)\n",
    "    # print(\"sentiment_count_with_interacts_col finished.\")\n",
    "    logging.info(\"sentiment_count_with_interacts_col finished.\")\n",
    "    comment_df.to_csv(\"2_sentiment_count_with_interacts_col.csv\", encoding=\"utf-8-sig\")\n",
    "    '''\n",
    "\n",
    "    '3. Labelling sentiment directly according to emoji.'\n",
    "    comment_df = label_with_emoji(comment_df, EMOJI_CSV)\n",
    "    comment_df.to_csv(\"test_csv/01_preprocess_label_with_emoji.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "    '4. pre-processing (data cleaning) for each comment string. '\n",
    "    comment_df = string_preprocessing_with_content_col(comment_df)\n",
    "    comment_df.to_csv(\"test_csv/02_preprocess_string_preprocessing_with_content_col.csv\", encoding=\"utf-8-sig\")\n",
    "    # print(\"string_preprocessing_with_content_col finished.\")\n",
    "    #logging.info(\"string_preprocessing_with_content_col finished.\")\n",
    "\n",
    "    # '4. pre-processing for each comment string according to different languages. '\n",
    "    # comment_df = string_preprocessing_with_content_col_for_multiple_lang(comment_df)\n",
    "    # print(\"string_preprocessing_with_content_col finished.\")\n",
    "    # logging.info(\"string_preprocessing_with_content_col finished.\")\n",
    "\n",
    "    '5. label empty string as neutral. '\n",
    "    comment_df = label_empty_string_as_neu(comment_df)\n",
    "    #print(\"label_empty_string_as_pos finished.\")\n",
    "    #logging.info(\"label_empty_string_as_pos finished.\")\n",
    "\n",
    "\n",
    "    '6.1 label one letter as neutral. '\n",
    "    comment_df = label_one_letter_as_neu(comment_df)\n",
    "    print(\"6.1 label empty string as neutral finished.\")\n",
    "    #logging.info(\"6.1 label_one_letter_as_neu finished.\")\n",
    "\n",
    "\n",
    "    '6.2 label iterate string as positive. '\n",
    "    comment_df = label_iterate_string_as_pos(comment_df)\n",
    "    print(\"6.2 label_iterate_string_as_pos finished.\")\n",
    "    #logging.info(\"6.2 label_iterate_string_as_pos finished.\")\n",
    "\n",
    "    '6.2 label digital and special charactors as neutral. '\n",
    "    comment_df = label_digital_special_charactors_as_neu(comment_df)\n",
    "    print(\"6.2 label_digital_special_charactors_as_neu finished.\")\n",
    "    #logging.info(\"6.2 label_digital_special_charactors_as_neu finished.\")\n",
    "\n",
    "    '6.2 label unsupported unicode string as IGNORE. '\n",
    "    comment_df = label_unsupported_unicode_as_IGNORE(comment_df)\n",
    "    print(\"6.3 label_unsupported_unicode_as_neu finished.\")\n",
    "    #logging.info(\"6.3 label_unsupported_unicode_as_neu finished.\")\n",
    "\n",
    "\n",
    "    '''\n",
    "    '6. Label all unlabelled comments that have the same content as labelled comments with the same label. '\n",
    "    comment_df = copy_senti_from_labelled_comments_of_same_content(comment_df)\n",
    "    # comment_df.to_csv(\"6. copy_senti_from_labelled_comments_of_same_content.csv\", encoding=\"utf-8-sig\")\n",
    "    # print(\"copy_senti_from_labelled_comments_of_same_content finished.\")\n",
    "    logging.info(\"copy_senti_from_labelled_comments_of_same_content finished.\")\n",
    "    '''\n",
    "\n",
    "    '7. Language Detection.'\n",
    "    comment_df = language_detection_with_content_col(comment_df)\n",
    "    comment_df.to_csv(\"test_csv/03_preprocess_language_detection_with_content_col.csv\", encoding=\"utf-8-sig\")\n",
    "    # print(\"language_detection_with_content_col finished.\")\n",
    "    #logging.info(\"language_detection_with_content_col finished.\")\n",
    "    # comment_df = comment_df[:100]\n",
    "    # print(\"comment_df\", comment_df.shape[0])\n",
    "    # a = language_detection_with_content_col_1(comment_df)\n",
    "    # b = language_detection_with_content_col_2(comment_df)\n",
    "    # c = language_detection_with_content_col_3(comment_df)\n",
    "    # d = language_detection_with_content_col_4(comment_df)\n",
    "    '''\n",
    "    '8. top_k post comments per post'\n",
    "    comment_df = Top_K_comments_per_post(comment_df)\n",
    "    comment_df.to_csv(\"7_01_Top_K_comments_per_post.csv\", encoding=\"utf-8-sig\")\n",
    "    print(\"7_01_Top_K_comments_per_post finished.\")\n",
    "    '''\n",
    "    return comment_df\n",
    "\n",
    "\n",
    "p = Pool( processes= NUM_PROC)\n",
    "each_len = int(comment_df.shape[0]//num_proc)+1\n",
    "# ret = p.map(pre_processing_and_pre_predict, [comment_df.loc[0:10, :] for i in range(num_proc)])\n",
    "ret = p.map(pre_processing_and_pre_predict, [comment_df.iloc[i*each_len:(i+1)*each_len, :] for i in range(num_proc)])\n",
    "p.close()\n",
    "p.join()\n",
    "# print(type(ret))\n",
    "# print(len(ret))\n",
    "# print(type(ret[0]))\n",
    "# print(ret[0].shape)\n",
    "# for index, item in enumerate(ret):\n",
    "#     item.to_csv(f\"ret_1214_{index}.csv\", encoding=\"utf-8-sig\")\n",
    "ret = pd.concat(ret, axis=0).reset_index(drop=True)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d221e2c56af5d695a1af40bb200d2da91d8004b744a2e05fcdd196049680e64b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('3.9.5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
